{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-like CNN Custom Estimator for MNIST, built with keras layers\n",
    "\n",
    "In this example, we'll look at how to build a Custom Estimator -- a CNN model -- using Keras layers to define the model. This version of the model is based on\n",
    "[this blog post](http://www.sas-programming.com/2017/09/a-vgg-like-cnn-for-fashion-mnist-with.html) --\n",
    "a [VGG-like]( http://www.robots.ox.ac.uk/~vgg) CNN (VGG is a deep convolutional network for object\n",
    "recognition developed and trained by Oxford's Visual Geometry Group, which achieved\n",
    "good performance on the ImageNet dataset.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, do some imports and define some variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you're running this notebook on colab**, download the `dataset.py` file from the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://raw.githubusercontent.com/amygdala/tensorflow-workshop/master/workshop_sections/mnist_series/mnist_cnn_custom_estimator/dataset.py\n",
    "ls -l dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convolutional Neural Network Custom Estimator for MNIST, built with tf.layers.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, LeakyReLU\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "MODEL_DIR = os.path.join(\"/tmp/tfmodels/mnist_cnn_estimator\",\n",
    "                          \"keras_vgglike\" + str(int(time.time())))\n",
    "# This is too short for proper training (especially with 'Fashion-MNIST'), \n",
    "# but we'll use it here to make the notebook quicker to run.\n",
    "NUM_STEPS = 5000\n",
    "\n",
    "print(\"using model dir: %s\" % MODEL_DIR)\n",
    "# Tensorflow version should be >=1.4.\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Fashion-MNIST\n",
    "\n",
    "Next, download Fashion-MNIST if you haven't already done so.  \n",
    "If you have, skip the next two cells and just edit `DATA_DIR` to point to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p fashion_mnist\n",
    "cd fashion_mnist\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
    "gunzip *\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -l fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the following value as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'fashion_mnist'\n",
    "# DATA_DIR = '/tmp/MNIST_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model function that will be used in constructing the Estimator.\n",
    "\n",
    "Note use of the Keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  input_layer = tf.reshape(features[\"pixels\"], [-1, 28, 28, 1])\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    K.set_learning_phase(True)\n",
    "  else:\n",
    "    K.set_learning_phase(False)\n",
    "\n",
    "  conv1 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "            input_shape=(28,28,1), activation='relu')(input_layer)\n",
    "  conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation='relu')(conv1)\n",
    "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "  dropout1 = Dropout(0.5)(pool1)\n",
    "  conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation='relu')(dropout1)\n",
    "  conv4 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"valid\", activation='relu')(conv3)\n",
    "  pool2 = MaxPooling2D(pool_size=(3, 3))(conv4)\n",
    "  dropout2 = Dropout(0.5)(pool2)\n",
    "  pool2_flat = Flatten()(dropout2)\n",
    "  dense1 = Dense(256)(pool2_flat)\n",
    "  lrelu = LeakyReLU()(dense1)\n",
    "  dropout3 = Dropout(0.5)(lrelu)\n",
    "  dense2 = Dense(256)(dropout3)\n",
    "  lrelu2 = LeakyReLU()(dense2)\n",
    "  logits = Dense(10, activation='linear')(lrelu2)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\n",
    "     \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")})\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\n",
    "        export_outputs={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_output})\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "  # Generate some summary info\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.histogram('conv1', conv1)\n",
    "  tf.summary.histogram('dense', dense1)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input functions for reading in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_input_fn(data_dir, batch_size=100):\n",
    "  \"\"\"Prepare data for training.\"\"\"\n",
    "\n",
    "  # When choosing shuffle buffer sizes, larger sizes result in better\n",
    "  # randomness, while smaller sizes use less memory. MNIST is a small\n",
    "  # enough dataset that we can easily shuffle the full epoch.\n",
    "  ds = dataset.train(data_dir)\n",
    "  ds = ds.cache().shuffle(buffer_size=50000).batch(batch_size=batch_size)\n",
    "\n",
    "  # Iterate through the dataset a set number of times\n",
    "  # during each training session.\n",
    "  ds = ds.repeat(40)\n",
    "  features = ds.make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]\n",
    "\n",
    "\n",
    "def eval_input_fn(data_dir, batch_size=100):\n",
    "  features = dataset.test(data_dir).batch(\n",
    "      batch_size=batch_size).make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model.  Pass the estimator object the input function to use, and some other config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "train_input = lambda: train_input_fn(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "eval_input = lambda: eval_input_fn(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=2000)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(train_input,\n",
    "                                  max_steps=NUM_STEPS,\n",
    "                                  hooks=[logging_hook]\n",
    "                                  )\n",
    "def serving_input_receiver_fn():\n",
    "    feature_tensor = tf.placeholder(tf.float32, [None, 784])\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        {'pixels': feature_tensor}, {'pixels': feature_tensor})\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('cnn_mnist', serving_input_receiver_fn)\n",
    "\n",
    "# While not shown here, we can also add a model 'exporter' to the EvalSpec.\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input,\n",
    "                                steps=NUM_STEPS,\n",
    "                                exporters=[exporter],\n",
    "                                name='cnn_mnist_keras'\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the `TrainSpec` and `EvalSpec` to pass to `tf.estimator.train_and_evaluate()`. As part of the `EvalSpec`, we define an Exporter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(mnist_classifier,\n",
    "                                train_spec,\n",
    "                                eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the characteristics of the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env MODEL_DIR=$MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "exported_model_dir=$(ls ${MODEL_DIR}/export/cnn_mnist)\n",
    "saved_model_cli show --dir ${MODEL_DIR}/export/cnn_mnist/${exported_model_dir} --tag serve --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at info about our training run in TensorBoard. \n",
    "\n",
    "**If you're running this notebook on colab**, you'll need to skip this step.\n",
    "\n",
    "Start up TensorBoard as follows in a new terminal window, pointing it to the MODEL_DIR. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "```\n",
    "$ tensorboard --logdir=<model_dir>\n",
    "```\n",
    "\n",
    "Try the following to compare across runs:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=/tmp/tfmodels\n",
    "```\n",
    "\n",
    "Or run the following (select Kernel --> Interrupt from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/tfmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
