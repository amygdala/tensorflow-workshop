{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# define a utility function for generating a new directory in which to save \n",
    "# model information, so multiple training runs don't stomp on each other.\n",
    "def get_new_path(name=\"\"):\n",
    "    base = os.path.abspath(\"/tmp/tfmodels/mnist_estimators\")\n",
    "    logpath = os.path.join(base, name + \"_\" + str(int(time.time())))\n",
    "    print(\"Logging to {}\".format(logpath))\n",
    "    return logpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "DATA_SETS = input_data.read_data_sets(\n",
    "    \"/tmp/MNIST_data\")\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    \"pixels\", shape=784)]\n",
    "\n",
    "m = tf.estimator.LinearClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        n_classes=10,\n",
    "        model_dir=get_new_path(\"linear\")\n",
    "    )\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'pixels': DATA_SETS.train.images},\n",
    "        y=DATA_SETS.train.labels.astype(np.int64),\n",
    "        batch_size=100,\n",
    "        num_epochs=3,\n",
    "        shuffle=True)\n",
    "m.train(input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'pixels': DATA_SETS.test.images},\n",
    "        y=DATA_SETS.test.labels.astype(np.int64),\n",
    "        batch_size=100,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "results = m.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bonus round 1: predictions\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(        \n",
    "        x={'pixels': DATA_SETS.test.images[5000:5005]},\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "predictions = m.predict(input_fn=predict_input_fn)\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(\"Predictions:    {} with probabilities {}\\n\".format(\n",
    "        prediction[\"classes\"], prediction[\"probabilities\"]))\n",
    "print('Expected answers values: {}'.format(\n",
    "    DATA_SETS.test.labels[5000:5005]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional Bonus round 2: visualizing our predictions.\n",
    "# This will fail if matplotlib is not installed. You can just skip it if so.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(5000,5005):\n",
    "    sample = np.reshape(DATA_SETS.test.images[i], (28,28))\n",
    "    plt.figure()\n",
    "    plt.title(\"labeled class {}\".format(DATA_SETS.test.labels[i]))\n",
    "    plt.imshow(sample, 'gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth walk through and exploration\n",
    "\n",
    "Let's look more closely at how to use TensorFlow's high-level Estimator classes to easily build a classifier with multiple hidden layers.\n",
    "\n",
    "Optionally [download Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist#get-the-data) if you haven't already.\n",
    "\n",
    "First, do some imports and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# comment out for less info during the training runs.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# confirm what version of TensorFlow you are running\n",
    "print('Running TensorFlow version {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set locations of data files\n",
    "MNIST_DATA_DIR = \"/tmp/MNIST_data\"\n",
    "# Edit the following to reflect where you put the Fashion-MNIST local dir,\n",
    "# if you want to experiment with Fashion-MNIST too.\n",
    "FASHION_DATA_DIR = \"your-fashion_mnist-dir\" \n",
    "\n",
    "# read in data, downloading first as necessary\n",
    "DATA_SETS = input_data.read_data_sets(MNIST_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "# call with generate_input_fn(DATA_SETS.train) or generate_input_fn(DATA_SETS.test)\n",
    "\n",
    "# These default settings will generate samples in the order of the file, forever.\n",
    "def generate_input_fn(dataset, \n",
    "                      epochs=None, \n",
    "                      shuffle=False, \n",
    "                      batch_size=BATCH_SIZE):\n",
    "    X = dataset.images\n",
    "    Y = dataset.labels.astype(numpy.int64)\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'pixels': X},\n",
    "        y=Y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=epochs,\n",
    "        shuffle=shuffle\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a function that adds a LinearClassifier and runs its `train()` method, which will train the model. Note that we didn't need to explicitly define a model graph or a training loop ourselves.  \n",
    "\n",
    "Once we've trained the model, we run the `evaluate()` method, which uses the trained model. To do this, it loads the most recent checkpointed model info available.  The model checkpoint(s) will be generated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_linear_classifier(num_steps, \n",
    "                                     logdir, \n",
    "                                     batch_size=BATCH_SIZE):\n",
    "    \"\"\"Run a linear classifier.\"\"\"\n",
    "\n",
    "    feature_columns = [tf.feature_column.numeric_column(\n",
    "            \"pixels\", shape=784)]\n",
    "    \n",
    "    classifier = tf.estimator.LinearClassifier(\n",
    "                    feature_columns=feature_columns, \n",
    "                    n_classes=10,\n",
    "                    model_dir=logdir\n",
    "                )\n",
    "    classifier.train(input_fn=generate_input_fn(\n",
    "        DATA_SETS.train, \n",
    "        epochs=3, \n",
    "        shuffle=True,\n",
    "        batch_size=batch_size),\n",
    "        steps=num_steps\n",
    "        )\n",
    "    \n",
    "    print(\"Finished training.\")\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    accuracy_score = classifier.evaluate(\n",
    "        input_fn=generate_input_fn(\n",
    "        DATA_SETS.test, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        epochs=1))['accuracy']\n",
    "    \n",
    "    print('Linear Classifier Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add a function that defines a `DNNClassifier`, and runs its `train()` method, which will train the model. Again note that we didn't need to explicitly define a model graph or a training loop ourselves.  \n",
    "\n",
    "Then after we've trained the model, we run the classifier's `evaluate()` method, which uses the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_dnn_classifier(num_steps, logdir, lr=.1, batch_size=40):\n",
    "    \"\"\"Run a DNN classifier.\"\"\"\n",
    "    feature_columns = [tf.feature_column.numeric_column(\n",
    "        \"pixels\", shape=784)]\n",
    "    \n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        n_classes=10,\n",
    "        hidden_units=[200, 100, 50],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=lr),\n",
    "        model_dir=logdir\n",
    "        )\n",
    "    # After you've done a training run with optimizer learning rate 0.1,\n",
    "        # change it to 0.5 and run the training again.  Use TensorBoard to take\n",
    "        # a look at the difference.  You can see both runs by pointing it to the\n",
    "        # parent model directory, which by default is:\n",
    "        #\n",
    "        #   tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "        \n",
    "    classifier.train(input_fn=generate_input_fn(\n",
    "        DATA_SETS.train, \n",
    "        epochs=3, \n",
    "        shuffle=True,\n",
    "        batch_size=batch_size),\n",
    "        steps=num_steps)\n",
    "\n",
    "    print(\"Finished running the deep training via the train() method\")\n",
    "    \n",
    "    accuracy_score = classifier.evaluate(input_fn=generate_input_fn(\n",
    "        DATA_SETS.test, batch_size=batch_size, shuffle=False, epochs=1))['accuracy']\n",
    "\n",
    "    print('DNN Classifier Accuracy: {0:f}'.format(accuracy_score))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the functions that define and train our classifiers. (It takes a moment to set up the input data queue before the training starts).\n",
    "\n",
    "Let's start with the LinearClassifier, which won't be very accurate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running Linear classifier ...\")\n",
    "define_and_run_linear_classifier(num_steps=500, \n",
    "                                 logdir=get_new_path(\"linear\"), \n",
    "                                 batch_size=40)\n",
    "# With 1000 steps and a batch size of 40, we see accuracy of approx 91% for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the DNN Classifier.  First, let's try it with a .1 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN classifier with .1 learning rate...\")\n",
    "classifier = define_and_run_dnn_classifier(\n",
    "    num_steps=2000, \n",
    "    logdir=get_new_path(\"deep01\"), \n",
    "    lr=.1)\n",
    "# With 2000 steps and a batch size of 40, we see accuracy of approx 95% on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: If you downloaded Fashion-MNIST, let's see what MNIST and Fashion-MNIST results look like side by side. Change the data directory param to point to your fashion-mnist dataset, and run the training again. Note that we're changing the model path (via the `get_new_path()` call) so that we don't write these results into the same directory as above.\n",
    "(If you didn't yet download Fashion-MNIST, this cell will give an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_SETS = input_data.read_data_sets(FASHION_DATA_DIR)\n",
    "print(\"Running DNN classifier with Fashion-MNIST data and a .1 learning rate...\")\n",
    "fclassifier = define_and_run_dnn_classifier(\n",
    "    num_steps=2000, \n",
    "    logdir=get_new_path(\"deep01f\"), \n",
    "    lr=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the Fashion-MNIST training, you can see that the accuracy is significantly worse. This dataset is harder! \n",
    "\n",
    "Now, let's run training with a .5 learning rate. This will use Fashion-MNIST if you reset the DATA_SET var above; otherwise, it will use regular MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN classifier with .5 learning rate...\")\n",
    "classifier5 = define_and_run_dnn_classifier(2000, \n",
    "                                            get_new_path(\"deep05\"), \n",
    "                                            lr=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this training run do better or worse than the .1 learning rate?\n",
    "\n",
    "To compare your results, start up TensorBoard as follows in a new terminal window. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "\n",
    "```sh\n",
    "$ tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "```\n",
    "Look for it at localhost:6006\n",
    "\n",
    "Or run the following (select Kernel --> Interrupt from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/tfmodels/mnist_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again make some predictions using our trained models.  Assuming you ran the Fashion-MNIST training, `DATA_SETS` now points to that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(DATA_SETS.test.labels[5000:5005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = DATA_SETS.test.images[5000:5005]\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(        \n",
    "        x={'pixels': X},\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "# if you did not run the Fashion-MNIST training, edit the following to point\n",
    "# to 'classifier' (the 'regular' MNIST model) instead of 'fclassifier'.\n",
    "predictions = fclassifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(\"Predictions:    {} with probabilities {}\\n\".format(prediction[\"classes\"], prediction[\"probabilities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the images we're predicting on. Again, skip these two cells if\n",
    "# matplotlib is not installed.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5000,5005):\n",
    "    sample = numpy.reshape(DATA_SETS.test.images[i], (28,28))\n",
    "    plt.figure()\n",
    "    plt.title(\"labeled class {}\".format(DATA_SETS.test.labels[i]))\n",
    "    plt.imshow(sample, 'gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
