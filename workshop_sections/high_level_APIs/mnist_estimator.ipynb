{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the TensorFlow Estimator APIs\n",
    "\n",
    "In this lab, we'll explore using TensorFlow's high-level [`tf.estimator`](https://www.tensorflow.org/programmers_guide/estimators) APIs, in order to easily build, train, evaluate, and use NN models.\n",
    "\n",
    "We'll do this via both the 'MNIST' dataset, and [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png), which is a direct drop-in replacement for the original MNIST dataset.\n",
    "(You can read more about it, and why it was created, [here](https://github.com/zalandoresearch/fashion-mnist). It is a more challenging dataset than 'regular' MNIST, which has become too easy these days.)\n",
    "\n",
    "The lab starts with a [`LinearClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier), then uses a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) with multiple hidden layers.\n",
    "\n",
    "As part of the lab, we'll explore what [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) can do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you're running this notebook on colab**, download the dataset.py file from the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://raw.githubusercontent.com/amygdala/tensorflow-workshop/master/workshop_sections/high_level_APIs/dataset.py\n",
    "ls -l dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some imports and check your version of TensorFlow.  It must be >=1.4, and ideally >=1.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import dataset\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# define a utility function for generating a new directory in which to save \n",
    "# model information, so multiple training runs don't stomp on each other.\n",
    "def get_new_path(name=\"\"):\n",
    "    base = os.path.abspath(\"/tmp/tfmodels/mnist_estimators\")\n",
    "    logpath = os.path.join(base, name + \"_\" + str(int(time.time())))\n",
    "    print(\"Logging to {}\".format(logpath))\n",
    "    return logpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started: A Linear Classifier\n",
    "\n",
    "First, let's build a LinearClassifier. \n",
    "\n",
    "We'll first build the models' input functions.\n",
    "\n",
    "We'll use [Datasets](https://www.tensorflow.org/get_started/datasets_quickstart) to manage the input to our model. The [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. \n",
    "[Datasets support highly scalable and performant input pipelines](https://www.tensorflow.org/performance/datasets_performance), and it is best practice to use them where possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/tmp/MNIST_data\"\n",
    "NUM_STEPS = 5000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def train_input_fn(data_dir, batch_size=100):\n",
    "  \"\"\"Prepare data for training.\"\"\"\n",
    "\n",
    "  # When choosing shuffle buffer sizes, larger sizes result in better\n",
    "  # randomness, while smaller sizes use less memory. MNIST is a small\n",
    "  # enough dataset that we can easily shuffle the full epoch.\n",
    "  ds = dataset.train(data_dir)\n",
    "  ds = ds.cache().shuffle(buffer_size=50000).batch(batch_size=batch_size)\n",
    "\n",
    "  # Iterate through the dataset a set number of times\n",
    "  # during each training session.\n",
    "  ds = ds.repeat(40)\n",
    "  features = ds.make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]\n",
    "\n",
    "\n",
    "def eval_input_fn(data_dir, batch_size=100):\n",
    "  features = dataset.test(data_dir).batch(\n",
    "      batch_size=batch_size).make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define and train the LinearClassifier model.\n",
    "Note that we didn't need to explicitly define a model graph or a training loop ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    \"pixels\", shape=784)]\n",
    "\n",
    "linear_classifier = tf.estimator.LinearClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        n_classes=10,\n",
    "        model_dir=get_new_path(\"linear\")\n",
    "    )\n",
    "\n",
    "train_input = lambda: train_input_fn(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "linear_classifier.train(input_fn=train_input, steps=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've trained the model, we'll run the evaluate() method, which uses the trained model. To do this, it loads the most recent checkpointed model info available. The model checkpoint(s) are generated during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_input = lambda: eval_input_fn(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "results = linear_classifier.evaluate(input_fn=eval_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the model accuracy is not great... we'll get back to that).\n",
    "\n",
    "We can also use the model to make a few predictions.   \n",
    "Note: If you wanted to actually deploy and serve the model, in order to support scalable predictions, you'd want to export it in a specific `SavedModel` format.  We'll get to that in a later example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "def predict_input_fn():\n",
    "  features = dataset.test(DATA_DIR).take(5).batch(batch_size=1).make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]\n",
    "\n",
    "predictions = linear_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(\"Predictions:    {} with probabilities {}\\n\".format(\n",
    "        prediction[\"classes\"], prediction[\"probabilities\"]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: What are the labels for these predictions?\n",
    "# This will fail if matplotlib is not installed. You can just skip it if so.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pred_next_item = dataset.test(DATA_DIR).take(5).batch(batch_size=1).make_one_shot_iterator().get_next()\n",
    "sess =  tf.Session()\n",
    "while True:\n",
    "  try:\n",
    "    item = sess.run(pred_next_item)\n",
    "    pred_label = item[1]\n",
    "    pred_image = item[0]\n",
    "    print(\"label: %s\" % pred_label)\n",
    "    sample = np.reshape(pred_image, (28,28))\n",
    "    plt.figure()\n",
    "    plt.imshow(sample, 'gray')\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNNClassifier: try a Deep Neural Net on the same task\n",
    "\n",
    "Next, let's see if a Deep Neural Net, with multiple hidden layers, does better at classification of these images.\n",
    "We'll use a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) with multiple hidden layers.\n",
    "\n",
    "First, do some imports and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/tmp/MNIST_data\"\n",
    "NUM_STEPS = 15000\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a `DNNClassifier`, and run its `train()` method, which will train the model. Again note that we didn't need to explicitly define a model graph or a training loop ourselves. \n",
    "\n",
    "You'll notice that this code looks much the same as that above, aside from a couple additional parameters when defining the model.\n",
    "We can use the same train and eval input functions as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try training the DNNClassifier with a .1 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    \"pixels\", shape=784)]\n",
    "\n",
    "LR = .1\n",
    "\n",
    "dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=10,\n",
    "        hidden_units=[128, 32],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=LR),\n",
    "        model_dir=get_new_path(\"dnn\")\n",
    "    )\n",
    "\n",
    "dnn_classifier.train(input_fn=train_input, steps=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll evaluate the trained model. Note the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "results = dnn_classifier.evaluate(input_fn=eval_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try using a .5 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR5 = .5\n",
    "\n",
    "dnn_classifier5 = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=10,\n",
    "        hidden_units=[128, 32],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=LR5),\n",
    "        model_dir=get_new_path(\"dnn5\")\n",
    "    )\n",
    "\n",
    "dnn_classifier5.train(input_fn=train_input, steps=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "results = dnn_classifier5.evaluate(input_fn=eval_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "To compare your results, let's start up TensorBoard! \n",
    "\n",
    "**Note**: If you're running this notebook on **colab, you will not be able to run TensorBoard from the notebook**, so you will need to skip this step.\n",
    "\n",
    "You can start it as follows in a new terminal window. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "\n",
    "```sh\n",
    "$ tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "```\n",
    "Look for it at localhost:6006\n",
    "\n",
    "Alternately, run the following (select Kernel --> Interrupt from the menu when you're done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/tfmodels/mnist_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST! and `tf.estimator.train_and_evaluate()`\n",
    "\n",
    "Next, let's look at our results with a data set that's harder: [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist#get-the-data).\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/amy-jo/images/fashion-mnist-sprite%20_sm.png\" width=\"40%\"\n",
    "         alt=\"Fashion MNIST\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already downloaded the Fashion-MNIST files, you can do so as follows. **If you've already downloaded them, you don't need to do so again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p fashion_mnist\n",
    "cd fashion_mnist\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
    "wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
    "gunzip *\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If wget is not installed on your machine, try **replacing** the `wget` lines with:\n",
    "```\n",
    "curl -o train-images-idx3-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
    "curl -o train-labels-idx1-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
    "curl -o t10k-images-idx3-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
    "curl -o t10k-labels-idx1-ubyte.gz http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
    "```\n",
    "or [download directly from the site](https://github.com/zalandoresearch/fashion-mnist#get-the-data).\n",
    "\n",
    "Confirm that everything looks okay. You want the files to be **unzipped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -l fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.estimator.train_and_evaluate()`\n",
    "\n",
    "TensorFlow’s version 1.4 release [introduced](https://cloud.google.com/blog/big-data/2018/02/easy-distributed-training-with-tensorflow-using-tfestimatortrain-and-evaluate-on-cloud-ml-engine) the [`tf.estimator.train_and_evaluate`](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate) function, which simplifies training, evaluation, and exporting of Estimator models. It abstracts away the details of distributed execution for training and evaluation, while also supporting consistent behavior across local/non-distributed and distributed configurations.\n",
    "\n",
    "For this example, we'll use `tf.estimator.train_and_evaluate` instead of making separate 'train' and 'evaluate' calls.\n",
    "To keep this example simple, we're not including model export.\n",
    "We'll show that in a later lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit path to directory as necessary\n",
    "FASHION_DATA_DIR = \"fashion_mnist\" \n",
    "\n",
    "train_input_fashion = lambda: train_input_fn(\n",
    "    FASHION_DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "eval_input_fashion = lambda: eval_input_fn(\n",
    "    FASHION_DATA_DIR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\n",
    "    \"pixels\", shape=784)]\n",
    "\n",
    "LR = .1\n",
    "\n",
    "run_config = tf.estimator.RunConfig()\n",
    "run_config = run_config.replace(model_dir=get_new_path(\"fashion_dnn\"))\n",
    "\n",
    "fashion_dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=10,\n",
    "        hidden_units=[128, 32],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=LR),\n",
    "        config=run_config\n",
    "    )\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fashion,\n",
    "                                  max_steps=NUM_STEPS\n",
    "                                  )\n",
    "\n",
    "# While not shown here, we can also add a model 'exporter' to the EvalSpec.\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input_fashion,\n",
    "                                steps=NUM_STEPS,\n",
    "                                name='fashion-eval'\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another metric to the estimator -- *recall* -- using Tensorflow's built-in metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_recall(labels, predictions):\n",
    "  return {'recall': tf.metrics.recall(labels, predictions['class_ids'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the recall metric to the estimator\n",
    "fashion_dnn_classifier = tf.contrib.estimator.add_metrics(fashion_dnn_classifier, my_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Before running `train_and_evaluate`, try adding a *precision* metric too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your new code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(fashion_dnn_classifier,\n",
    "                                train_spec,\n",
    "                                eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the accuracy is significantly worse than with 'regular' MNIST. This dataset is harder! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again make some predictions using our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "def predict_input_fn():\n",
    "  features = dataset.test(FASHION_DATA_DIR).skip(5575).take(5).batch(batch_size=1).make_one_shot_iterator().get_next()\n",
    "  return {'pixels': features[0]}, features[1]\n",
    "\n",
    "predictions = fashion_dnn_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(\"Predictions:    {} with probabilities {}\\n\".format(\n",
    "        prediction[\"classes\"], prediction[\"probabilities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: What are the labels for these predictions?\n",
    "# This will fail if matplotlib is not installed. You can just skip it if so.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pred_next_item = dataset.test(FASHION_DATA_DIR).skip(5575).take(5).batch(batch_size=1).make_one_shot_iterator().get_next()\n",
    "sess =  tf.Session()\n",
    "while True:\n",
    "  try:\n",
    "    item = sess.run(pred_next_item)\n",
    "    pred_label = item[1]\n",
    "    pred_image = item[0]\n",
    "    print(\"label: %s\" % pred_label)\n",
    "    sample = np.reshape(pred_image, (28,28))\n",
    "    plt.figure()\n",
    "    plt.imshow(sample, 'gray')\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Let's compare results again using TensorBoard.\n",
    "\n",
    "**Note**: If you're running this notebook on **colab**, you will not be able to run TensorBoard from the notebook, so you will need to skip this step.\n",
    "\n",
    "If TensorBoard is still running in a terminal window from before, it should pick up the new data automatically, since we pointed it to the parent directory of all the training runs we're doing. (If it doesn't seem to have done so, just reload).\n",
    "\n",
    "Otherwise, start up TensorBoard as follows in a new terminal window. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "\n",
    "```sh\n",
    "$ tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "```\n",
    "Look for it at localhost:6006\n",
    "\n",
    "Or run the following (select Kernel --> Interrupt from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/tfmodels/mnist_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try training a DNNClassifier model, using Fashion MNIST, with a .5 learning rate. \n",
    "Does this training do better or worse than the .1 learning rate on the Fashion MNIST dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR5 = .5\n",
    "# Your edits here\n",
    "fashion_dnn_classifier5 = ...\n",
    "\n",
    "...\n",
    "\n",
    "tf.estimator.train_and_evaluate(fashion_dnn_classifier5,\n",
    "                                train_spec,\n",
    "                                eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this training run do better or worse than the .1 learning rate on fashion mnist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
